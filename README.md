
Each layer and activation function is implemented manually using **PyTorch tensors** with `requires_grad=True`.

---

## ðŸ§© Implementation Details

- **Framework:** PyTorch  
- **Activations Used:** ReLU, Sigmoid, Tanh  
- **Operations:** All done manually (`torch.relu`, `torch.sigmoid`, `torch.tanh`, etc.)  
- **Gradients:** Computed using `output.backward()`  
- **Printed Outputs:** Intermediate activations and final gradients

---

## ðŸš€ How to Run

1. **Clone the repository:**
   ```bash
   git clone https://github.com/<your-username>/pytorch-computational-graph-assignment1.git
   cd pytorch-computational-graph-assignment1
